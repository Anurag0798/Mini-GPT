{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a356d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\", 'r') as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec613de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 5000, oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts([text_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86695539",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b72eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05502e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences([text_data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec10d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a93b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f415cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer.pkl\", 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 100\n",
    "\n",
    "def create_dataset(seq, window_size = 100):\n",
    "    input, label = [], []\n",
    "    for i in range(len(seq) - window_size):\n",
    "        input.append(seq[i: i+window_size])\n",
    "        label.append(seq[i+1: i+window_size+1])\n",
    "    \n",
    "    return np.array(input), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1af3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = create_dataset(sequence, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd149e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01aa59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype = tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c893bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(embed_dim, num_heads, ff_dim, dropout = 0.1):\n",
    "\n",
    "    input = layers.Input(shape = (None, embed_dim))\n",
    "    attn_output = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)(input, input)\n",
    "    attn_output = layers.Dropout(dropout)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon = 1e-6)(input + attn_output)\n",
    "    \n",
    "    ffn = tf.keras.Sequential([\n",
    "        layers.Dense(ff_dim, activation = 'relu'),\n",
    "        layers.Dense(embed_dim)\n",
    "    ])\n",
    "    \n",
    "    ffn_output = ffn(out1)\n",
    "    ffn_output = layers.Dropout(dropout)(ffn_output)\n",
    "    out2 = layers.LayerNormalization(epsilon = 1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return tf.keras.Model(inputs = input, outputs = out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73320b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block_with_mask_head(embed_dim, num_heads, ff_dim, dropout = 0.1, use_mask = False):\n",
    "\n",
    "    input = layers.Input(shape = (None, embed_dim))\n",
    "    mask_input = layers.Input(shape = (1, None, None), name = \"attention_mask\") if use_mask else None\n",
    "    attn_output = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)(input, input, attention_mask = mask_input if use_mask else None)\n",
    "    attn_output = layers.Dropout(dropout)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon = 1e-6)(input + attn_output)\n",
    "    \n",
    "    ffn = tf.keras.Sequential([\n",
    "        layers.Dense(ff_dim, activation = 'relu'),\n",
    "        layers.Dense(embed_dim)\n",
    "    ])\n",
    "    \n",
    "    ffn_output = ffn(out1)\n",
    "    ffn_output = layers.Dropout(dropout)(ffn_output)\n",
    "    out2  =  layers.LayerNormalization(epsilon = 1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return tf.keras.Model(inputs = [input, mask_input], outputs = out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc327cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocb_size = 5000\n",
    "max_seq_length = 100\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 1024\n",
    "num_layers = 4\n",
    "batch_size = 32\n",
    "epoch = 1\n",
    "\n",
    "def build_gpt_model():\n",
    "    input = layers.Input(shape = (max_seq_length,))\n",
    "    x = layers.Embedding(input_dim = vocb_size, output_dim = embed_dim)(input)\n",
    "    x = PositionalEncoding(max_seq_length, embed_dim)(x)\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_block(embed_dim = embed_dim, num_heads = num_heads, ff_dim = ff_dim)(x)\n",
    "        \n",
    "    output = layers.Dense(vocb_size, activation = 'softmax')(x)\n",
    "    return tf.keras.Model(inputs = input, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_gpt_model()\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_data, y_data, batch_size = batch_size, epochs = epoch, validation_split = .1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fa7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015db074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_logits(logits, temperature = 1.0, top_k = 10):\n",
    "    logits = logits / temperature\n",
    "    sorted_indices = np.argsort(logits)[::-1]\n",
    "    sorted_logits = logits[sorted_indices]\n",
    "    \n",
    "    top_k = min(top_k, len(sorted_logits))\n",
    "    top_k_indices = sorted_indices[:top_k]\n",
    "    top_k_logits = sorted_logits[:top_k]\n",
    "    top_k_probs = np.exp(top_k_logits) / np.sum(np.exp(top_k_logits))\n",
    "\n",
    "    return np.random.choice(top_k_indices, p = top_k_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_seq_len, num_tokens = 50, temperature = 1.0, top_k = 10):\n",
    "    input_seq = tokenizer.texts_to_sequences([prompt])[0]\n",
    "    input_seq = input_seq[-max_seq_len:]\n",
    "    padded = pad_sequences([input_seq], maxlen = max_seq_len)\n",
    "\n",
    "    for _ in range(num_tokens):\n",
    "        predictions = model(padded, training = False).numpy()\n",
    "        next_token_logits = predictions[0][-1]\n",
    "        next_token = sample_from_logits(next_token_logits, temperature, top_k)\n",
    "\n",
    "        input_seq.append(next_token)\n",
    "        input_seq = input_seq[-max_seq_len:]\n",
    "        padded = pad_sequences([input_seq], maxlen = max_seq_len)\n",
    "\n",
    "    return tokenizer.sequences_to_texts([input_seq])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec80af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = model       # change if needed / or model can be loaded from the physically stored location on disk as well if required\n",
    "TOKENIZER_PATH = tokenizer      # tokenizer saved with pickle\n",
    "MAX_SEQ_LEN = 100                     # should match training\n",
    "NUM_TOKENS_TO_GENERATE = 50\n",
    "TEMPERATURE = 1.0\n",
    "TOP_K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6551537",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\"Enter your testing prompt here\", model = MODEL_PATH, tokenizer = TOKENIZER_PATH, max_seq_len = MAX_SEQ_LEN, num_tokens = NUM_TOKENS_TO_GENERATE, temperature = TEMPERATURE, top_k = TOP_K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
